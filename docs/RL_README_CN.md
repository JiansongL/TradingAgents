# ğŸ¤– å¼ºåŒ–å­¦ä¹ å¢å¼ºç‰ˆ TradingAgents

## ğŸ¯ æ¦‚è¿°

æˆ‘ä»¬å·²ç»æˆåŠŸå°†**å¼ºåŒ–å­¦ä¹ (RL)æ¨¡å‹**é›†æˆåˆ° TradingAgents ç³»ç»Ÿä¸­ï¼ç°åœ¨ç³»ç»Ÿå¯ä»¥ï¼š

1. æ”¶é›†æ‰€æœ‰åˆ†ææ•°æ®ï¼ˆæŠ€æœ¯æŒ‡æ ‡ã€åŸºæœ¬é¢ã€æ–°é—»ã€æƒ…ç»ªã€æœŸæƒGreeksç­‰ï¼‰
2. å°†æ•°æ®ç¼–ç ä¸ºçŠ¶æ€å‘é‡
3. ä½¿ç”¨Deep Q-Network (DQN)å­¦ä¹ æœ€ä¼˜äº¤æ˜“ç­–ç•¥
4. **è¾“å‡ºç›ˆåˆ©æ¦‚ç‡**ï¼ˆæ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼‰
5. é€šè¿‡æ¢ç´¢å’Œåˆ©ç”¨å­¦ä¹ ï¼Œå¥–åŠ±æœºåˆ¶ä¸ºï¼šç›ˆåˆ© = +1ï¼ŒäºæŸ = -1

## âœ¨ æ–°å¢åŠŸèƒ½

### 1. **çŠ¶æ€ç¼–ç å™¨ (StateEncoder)**
å°†æ‰€æœ‰åˆ†ææ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€çš„çŠ¶æ€å‘é‡ï¼š
- æŠ€æœ¯æŒ‡æ ‡ï¼ˆRSI, MACD, SMA, EMA, ATRç­‰ï¼‰
- åŸºæœ¬é¢æŒ‡æ ‡ï¼ˆPE, EPS, Revenueç­‰ï¼‰
- æ–°é—»æƒ…ç»ªåˆ†æ•°
- ç¤¾äº¤åª’ä½“æƒ…ç»ª
- æœŸæƒæ•°æ®ï¼ˆDelta, Gamma, Theta, Vega, Rho, IVç­‰ï¼‰
- å¸‚åœºçŠ¶æ€

### 2. **å¥–åŠ±è®¡ç®—å™¨ (RewardCalculator)**
å¤šç§å¥–åŠ±è®¡ç®—æ–¹å¼ï¼š
- **åŸºç¡€å¥–åŠ±**ï¼šç›ˆåˆ© = +1ï¼ŒäºæŸ = -1
- **é£é™©è°ƒæ•´**ï¼šè€ƒè™‘æ³¢åŠ¨ç‡å’Œä»“ä½å¤§å°
- **æœŸæƒç‰¹å®š**ï¼šè€ƒè™‘Greekså’Œæ—¶é—´è¡°å‡
- **å¤æ™®æ¯”ç‡**ï¼šé£é™©è°ƒæ•´åæ”¶ç›Š
- **èƒœç‡å¥–åŠ±**ï¼šåŸºäºå†å²è¡¨ç°

### 3. **äº¤æ˜“ç¯å¢ƒ (TradingEnvironment)**
gymé£æ ¼çš„RLç¯å¢ƒï¼š
- `reset()`: åˆå§‹åŒ–ç¯å¢ƒ
- `step()`: æ‰§è¡ŒåŠ¨ä½œå¹¶è·å¾—å¥–åŠ±
- æ”¯æŒè‚¡ç¥¨å’ŒæœŸæƒäº¤æ˜“
- è‡ªåŠ¨è¿½è¸ªäº¤æ˜“å†å²å’Œç»Ÿè®¡

### 4. **RLä»£ç† (RLTradingAgent)**
ä½¿ç”¨Deep Q-Network (DQN)ï¼š
- 3å±‚ç¥ç»ç½‘ç»œæ¶æ„
- Experience Replay Buffer
- Target Network
- Epsilon-greedyæ¢ç´¢ç­–ç•¥
- **è¾“å‡ºç›ˆåˆ©æ¦‚ç‡**

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install torch matplotlib
# æˆ–è€…
pip install -r requirements.txt
```

### åŸºç¡€ä½¿ç”¨ - è·å–ç›ˆåˆ©æ¦‚ç‡

```python
from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.rl.rl_agent import RLTradingAgent
from tradingagents.rl.rl_state_encoder import StateEncoder
from tradingagents.default_config import DEFAULT_CONFIG

# åˆå§‹åŒ–
config = DEFAULT_CONFIG.copy()
ta = TradingAgentsGraph(debug=True, config=config)

# åˆå§‹åŒ–RLç»„ä»¶
state_encoder = StateEncoder(state_dim=128)
rl_agent = RLTradingAgent(state_dim=128, action_dim=3)

# åˆ†æè‚¡ç¥¨
agent_state, decision = ta.propagate("NVDA", "2024-05-10")

# ç¼–ç çŠ¶æ€
state_vector = state_encoder.encode(agent_state)

# è·å–ç›ˆåˆ©æ¦‚ç‡
profit_probs = rl_agent.get_all_action_probabilities(state_vector)

print("ç›ˆåˆ©æ¦‚ç‡:")
print(f"  ä¹°å…¥ (BUY):  {profit_probs['BUY']:.2%}")
print(f"  æŒæœ‰ (HOLD): {profit_probs['HOLD']:.2%}")
print(f"  å–å‡º (SELL): {profit_probs['SELL']:.2%}")
```

### ä½¿ç”¨é›†æˆç³»ç»Ÿ

```python
from rl_enhanced_example import RLEnhancedTradingSystem

# åˆ›å»ºå¢å¼ºç³»ç»Ÿ
system = RLEnhancedTradingSystem(
    use_rl=True,
    rl_model_path="./models/rl_trading_agent.pth"  # å¯é€‰ï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
)

# åˆ†æå¹¶è·å–ç›ˆåˆ©æ¦‚ç‡
result = system.analyze_and_predict("NVDA", "2024-05-10")

print(f"ä»£ç†å»ºè®®: {result['agent_decision']}")
print(f"RLæ¨è: {result['rl_recommendation']}")
print(f"ç›ˆåˆ©æ¦‚ç‡: {result['profit_probabilities']}")
print(f"æœ€ç»ˆå†³ç­–: {result['final_decision']}")
```

### è®­ç»ƒRLæ¨¡å‹

```python
from tradingagents.rl.rl_trainer import RLTrainer

# åˆ›å»ºè®­ç»ƒå™¨
trainer = RLTrainer(
    episodes=1000,
    max_steps_per_episode=100,
)

# è®­ç»ƒ
trained_agent = trainer.train(
    tickers=["NVDA", "AAPL", "TSLA"],
    save_path="./models/rl_trading_agent.pth",
)

# æŸ¥çœ‹è®­ç»ƒç»Ÿè®¡
stats = trained_agent.get_training_stats()
print(stats)
```

## ğŸ“Š ç³»ç»Ÿæ¶æ„

### æ•°æ®æµ

```
å¤šæ™ºèƒ½ä½“åˆ†æ
    â”œâ”€â”€ æŠ€æœ¯åˆ†æå¸ˆ â†’ æŠ€æœ¯æŒ‡æ ‡
    â”œâ”€â”€ åŸºæœ¬é¢åˆ†æå¸ˆ â†’ è´¢åŠ¡æ•°æ®
    â”œâ”€â”€ æ–°é—»åˆ†æå¸ˆ â†’ æ–°é—»æƒ…ç»ª
    â”œâ”€â”€ ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ â†’ ç¤¾äº¤æƒ…ç»ª
    â””â”€â”€ æœŸæƒåˆ†æå¸ˆ â†’ Greeks, IV
              â†“
        çŠ¶æ€ç¼–ç å™¨ (State Encoder)
              â†“
      [state_vector: 128ç»´]
              â†“
       RLä»£ç† (DQN Model)
              â†“
    ç›ˆåˆ©æ¦‚ç‡ (Profit Probability)
    - BUY: 45%
    - HOLD: 30%
    - SELL: 25%
              â†“
         æœ€ç»ˆå†³ç­–
```

### RLæ¨¡å‹æ¶æ„

```
Input Layer (128ç»´çŠ¶æ€å‘é‡)
    â†“
Dense Layer (256) + ReLU + Dropout
    â†“
Dense Layer (128) + ReLU + Dropout
    â†“
Dense Layer (64) + ReLU + Dropout
    â†“
Output Layer (3: BUY, HOLD, SELL)
    â†“
Softmax â†’ ç›ˆåˆ©æ¦‚ç‡
```

## ğŸ“ ç‰¹å¾è¯´æ˜

### çŠ¶æ€å‘é‡åŒ…å«çš„ç‰¹å¾

#### 1. æŠ€æœ¯æŒ‡æ ‡ (9ä¸ªç‰¹å¾)
- RSI (0-1æ ‡å‡†åŒ–)
- MACD
- 50æ—¥SMA
- 200æ—¥SMA
- 10æ—¥EMA
- æˆäº¤é‡
- æ³¢åŠ¨ç‡
- ATR
- è¶‹åŠ¿æ–¹å‘ (1=çœ‹æ¶¨, -1=çœ‹è·Œ, 0=ä¸­æ€§)

#### 2. åŸºæœ¬é¢æŒ‡æ ‡ (6ä¸ªç‰¹å¾)
- PEæ¯”ç‡
- EPS
- è¥æ”¶
- åˆ©æ¶¦ç‡
- è´Ÿå€ºç‡
- ROE

#### 3. æ–°é—»ç‰¹å¾ (3ä¸ªç‰¹å¾)
- æ–°é—»æƒ…ç»ª (-1åˆ°1)
- æ–°é—»æ•°é‡
- å¸‚åœºå½±å“åŠ›

#### 4. ç¤¾äº¤æƒ…ç»ª (2ä¸ªç‰¹å¾)
- ç¤¾äº¤åª’ä½“æƒ…ç»ª
- æƒ…ç»ªå¼ºåº¦

#### 5. æœŸæƒç‰¹å¾ (8ä¸ªç‰¹å¾)
- Delta
- Gamma
- Theta
- Vega
- Rho
- éšå«æ³¢åŠ¨ç‡
- çœ‹æ¶¨çœ‹è·Œæ¯”ç‡
- ç­–ç•¥ç±»å‹

## ğŸ‹ï¸ è®­ç»ƒè¿‡ç¨‹

### å¥–åŠ±æœºåˆ¶

```python
# åŸºç¡€å¥–åŠ±
ç›ˆåˆ© â†’ +1
äºæŸ â†’ -1

# æŒ‰ç…§ç›ˆåˆ©å¹…åº¦ç¼©æ”¾
reward *= abs(æ”¶ç›Šç‡) * 10

# å‡å»äº¤æ˜“æˆæœ¬
reward -= 0.01

# é£é™©è°ƒæ•´
reward -= æ³¢åŠ¨ç‡ * 0.1
```

### è®­ç»ƒå‚æ•°

```python
{
    "episodes": 1000,           # è®­ç»ƒå›åˆæ•°
    "max_steps": 100,           # æ¯å›åˆæœ€å¤§æ­¥æ•°
    "learning_rate": 0.001,     # å­¦ä¹ ç‡
    "gamma": 0.99,              # æŠ˜æ‰£å› å­
    "epsilon_start": 1.0,       # åˆå§‹æ¢ç´¢ç‡
    "epsilon_end": 0.01,        # æœ€ç»ˆæ¢ç´¢ç‡
    "epsilon_decay": 0.995,     # æ¢ç´¢è¡°å‡
    "batch_size": 64,           # æ‰¹å¤§å°
    "buffer_capacity": 10000,   # ç»éªŒå›æ”¾å®¹é‡
}
```

## ğŸ“ˆ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: å•ä¸ªè‚¡ç¥¨åˆ†æ

```python
system = RLEnhancedTradingSystem(use_rl=True)
result = system.analyze_and_predict("NVDA", "2024-05-10")

print(f"ç›ˆåˆ©æ¦‚ç‡ - BUY:  {result['profit_probabilities']['BUY']:.2%}")
print(f"ç›ˆåˆ©æ¦‚ç‡ - HOLD: {result['profit_probabilities']['HOLD']:.2%}")
print(f"ç›ˆåˆ©æ¦‚ç‡ - SELL: {result['profit_probabilities']['SELL']:.2%}")
```

### ç¤ºä¾‹2: æœŸæƒäº¤æ˜“

```python
config = DEFAULT_CONFIG.copy()
config["trading_mode"] = "options"
config["options_enabled"] = True

system = RLEnhancedTradingSystem(config=config, use_rl=True)
result = system.analyze_and_predict("AAPL", "2024-05-10")

# RLæ¨¡å‹ä¼šè€ƒè™‘æœŸæƒGreeksæ¥è®¡ç®—ç›ˆåˆ©æ¦‚ç‡
```

### ç¤ºä¾‹3: æŠ•èµ„ç»„åˆä¼˜åŒ–

```python
system = RLEnhancedTradingSystem(use_rl=True)

portfolio = system.get_portfolio_recommendation(
    tickers=["NVDA", "AAPL", "TSLA", "MSFT"],
    date="2024-05-10",
    capital=100000.0,
)

# æ ¹æ®RLæ¨¡å‹çš„ç›ˆåˆ©æ¦‚ç‡åˆ†é…èµ„é‡‘
for position in portfolio['positions']:
    print(f"{position['ticker']}: ${position['allocation']:,.2f}")
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

```python
from tradingagents.rl.rl_reward import RewardCalculator

reward_calc = RewardCalculator(
    profit_reward=1.0,
    loss_penalty=-1.0,
    transaction_cost=0.01,
    risk_penalty_weight=0.1,
)

# æœŸæƒå¥–åŠ±
reward = reward_calc.calculate_options_reward(
    action="BUY",
    strategy="call",
    premium=5.0,
    pnl=10.0,
    max_loss=5.0,
    greeks={'delta': 0.6, 'theta': -0.05}
)
```

### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
rl_agent = RLTradingAgent(state_dim=128, action_dim=3)
rl_agent.load_model("./models/rl_trading_agent.pth")

# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
probabilities = rl_agent.get_all_action_probabilities(state)
```

### æŒç»­è®­ç»ƒ

```python
# åŠ è½½ç°æœ‰æ¨¡å‹
rl_agent.load_model("./models/checkpoint_ep500.pth")

# ç»§ç»­è®­ç»ƒ
trainer = RLTrainer(episodes=500)
trainer.rl_agent = rl_agent
trainer.train(save_path="./models/checkpoint_ep1000.pth")
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

è®­ç»ƒåå¯ä»¥æŸ¥çœ‹ä»¥ä¸‹æŒ‡æ ‡ï¼š

```python
stats = rl_agent.get_training_stats()

print(f"æ€»å›åˆæ•°: {stats['total_episodes']}")
print(f"å¹³å‡å¥–åŠ±: {stats['avg_reward']:.2f}")
print(f"å¹³å‡æŸå¤±: {stats['avg_loss']:.4f}")
print(f"æ¢ç´¢ç‡: {stats['epsilon']:.3f}")
print(f"è®­ç»ƒæ­¥æ•°: {stats['training_steps']}")
print(f"ç¼“å†²åŒºå¤§å°: {stats['buffer_size']}")
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ•°æ®è´¨é‡
- ç¡®ä¿åˆ†ææŠ¥å‘ŠåŒ…å«è¶³å¤Ÿçš„æ•°å€¼æ•°æ®
- æ ‡å‡†åŒ–è¾“å…¥ç‰¹å¾
- å¤„ç†ç¼ºå¤±å€¼

### 2. è®­ç»ƒç­–ç•¥
- ä»å°æ•°æ®é›†å¼€å§‹è®­ç»ƒ
- é€æ­¥å¢åŠ å¤æ‚åº¦
- å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
- ç›‘æ§è¿‡æ‹Ÿåˆ

### 3. è¶…å‚æ•°è°ƒä¼˜
- è°ƒæ•´å­¦ä¹ ç‡
- ä¼˜åŒ–ç½‘ç»œæ¶æ„
- å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
- è°ƒæ•´å¥–åŠ±å‡½æ•°

### 4. ç”Ÿäº§éƒ¨ç½²
- ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
- å®šæœŸé‡æ–°è®­ç»ƒ
- A/Bæµ‹è¯•æ–°æ¨¡å‹
- ç›‘æ§å®æ—¶æ€§èƒ½

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ¨¡å‹ä¸ç¡®å®šæ€§**: æœªè®­ç»ƒçš„æ¨¡å‹è¾“å‡ºéšæœºæ¦‚ç‡
2. **æ•°æ®éœ€æ±‚**: éœ€è¦å¤§é‡å†å²æ•°æ®æ¥è®­ç»ƒ
3. **è®¡ç®—èµ„æº**: è®­ç»ƒå¯èƒ½éœ€è¦GPUåŠ é€Ÿ
4. **å¸‚åœºå˜åŒ–**: æ¨¡å‹éœ€è¦å®šæœŸé‡æ–°è®­ç»ƒ
5. **é£é™©è­¦å‘Š**: è¿™æ˜¯ç ”ç©¶å·¥å…·ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®

## ğŸ”® æœªæ¥æ”¹è¿›

- [ ] æ”¯æŒæ›´å¤šRLç®—æ³•ï¼ˆA3C, PPO, SACï¼‰
- [ ] å¤šæ­¥é¢„æµ‹ï¼ˆé¢„æµ‹æœªæ¥Næ­¥ï¼‰
- [ ] å¯¹æŠ—è®­ç»ƒï¼ˆå¤„ç†å¸‚åœºå¯¹æ‰‹ï¼‰
- [ ] è¿ç§»å­¦ä¹ ï¼ˆè·¨èµ„äº§å­¦ä¹ ï¼‰
- [ ] é›†æˆå­¦ä¹ ï¼ˆå¤šæ¨¡å‹ensembleï¼‰
- [ ] å®æ—¶åœ¨çº¿å­¦ä¹ 
- [ ] é£é™©æ•æ„Ÿçš„RL
- [ ] åˆ†å±‚å¼ºåŒ–å­¦ä¹ 

## ğŸ“š ç›¸å…³èµ„æº

- [Deep Q-Networkè®ºæ–‡](https://www.nature.com/articles/nature14236)
- [PyTorchæ•™ç¨‹](https://pytorch.org/tutorials/)
- [å¼ºåŒ–å­¦ä¹ å…¥é—¨](http://www.incompleteideas.net/book/the-book.html)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼å¯ä»¥æ”¹è¿›çš„æ–¹å‘ï¼š
- æ·»åŠ æ–°çš„ç‰¹å¾å·¥ç¨‹
- å®ç°å…¶ä»–RLç®—æ³•
- ä¼˜åŒ–å¥–åŠ±å‡½æ•°
- æ”¹è¿›è®­ç»ƒæ•ˆç‡
- æ·»åŠ å¯è§†åŒ–å·¥å…·

---

**å…è´£å£°æ˜**: å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„è¾“å‡ºä»…ä¾›ç ”ç©¶å‚è€ƒã€‚å®é™…äº¤æ˜“å…·æœ‰é«˜é£é™©ï¼Œå¯èƒ½å¯¼è‡´å…¨éƒ¨æŠ•èµ„æŸå¤±ã€‚è¯·è°¨æ…ä½¿ç”¨ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚
